name: Lighthouse CI

on:
  schedule:
    - cron: "0 8 * * *" # Run every day at 8 AM
    - cron: "0 18 * * *" # Run every day at 6 PM

# Allow cancellation of previous runs in the same PR/branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: write
  pull-requests: write

jobs:
  lighthouse-ci:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      matrix:
        page:
          - name: homepage
            url: https://curalife.com/
          - name: product
            url: https://curalife.com/products/curalin
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18"

      - name: Install dependencies
        run: |
          npm install -g @lhci/cli puppeteer lighthouse puppeteer-screenshot-cli

      - name: Set Chrome flags
        run: |
          echo "CHROME_FLAGS=--no-sandbox --disable-dev-shm-usage --disable-gpu --headless" >> $GITHUB_ENV

      - name: Setup results directory
        id: results-dir
        run: |
          RESULTS_DIR="${{ matrix.page.name }}-lighthouse-results"
          echo "results_dir=$RESULTS_DIR" >> $GITHUB_OUTPUT
          mkdir -p $RESULTS_DIR

      - name: Make scripts executable
        run: |
          chmod +x .github/workflows/scripts/make-scripts-executable.sh
          .github/workflows/scripts/make-scripts-executable.sh

      - name: Run Lighthouse CI
        id: lighthouse
        run: |
          echo "Running Lighthouse on ${{ matrix.page.url }}..."
          .github/workflows/scripts/run-lighthouse.sh "${{ matrix.page.url }}" "${{ matrix.page.name }}" "${{ steps.results-dir.outputs.results_dir }}"
        continue-on-error: true

      - name: Install jq for results processing
        run: sudo apt-get update && sudo apt-get install -y jq bc

      - name: Process Lighthouse results
        id: scores
        run: |
          # Set current date for outputs
          CURRENT_DATE=$(date +"%Y-%m-%d")
          echo "report_date=$CURRENT_DATE" >> $GITHUB_OUTPUT

          .github/workflows/scripts/process-results.sh "${{ matrix.page.name }}" "${{ steps.results-dir.outputs.results_dir }}" "$CURRENT_DATE"
        continue-on-error: true

      - name: Save reports to performance-reports directory
        if: steps.scores.outputs.has_results == 'true'
        run: |
          .github/workflows/scripts/save-reports.sh "${{ matrix.page.name }}" "${{ steps.results-dir.outputs.results_dir }}" "${{ steps.scores.outputs.report_date }}"
        continue-on-error: true

      - name: Format Lighthouse results
        if: steps.scores.outputs.has_results == 'true'
        run: |
          .github/workflows/scripts/format-summary.sh "${{ matrix.page.name }}"
        continue-on-error: true

      # Upload results for this page as an artifact
      - name: Upload individual page report
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.page.name }}-lighthouse-report
          path: performance-reports/
          retention-days: 30
        continue-on-error: true

    outputs:
      report_date: ${{ steps.scores.outputs.report_date }}

  create-dashboard:
    needs: lighthouse-ci
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Make scripts executable
        run: |
          chmod +x .github/workflows/scripts/make-scripts-executable.sh
          .github/workflows/scripts/make-scripts-executable.sh

      - name: Create performance-reports directory
        run: mkdir -p performance-reports

      # Download all page reports to combine them
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: performance-reports-downloads
          pattern: "*-lighthouse-report"
          merge-multiple: true
        continue-on-error: true

      # Copy all downloaded artifacts to the performance-reports directory
      - name: Organize reports
        run: |
          echo "Listing downloaded artifacts:"
          ls -la performance-reports-downloads || echo "No downloads directory found"

          # Create the performance reports directory if it doesn't exist
          mkdir -p performance-reports

          # Only try to copy if the downloads directory exists and has files
          if [ -d "performance-reports-downloads" ] && [ "$(ls -A performance-reports-downloads 2>/dev/null)" ]; then
            echo "Copying artifacts to performance-reports directory"
            cp -R performance-reports-downloads/* performance-reports/ || echo "No files to copy"
          else
            echo "No artifacts found to copy"
            # Create sample data with a simple approach instead of heredoc
            mkdir -p performance-reports/$(date +"%Y-%m-%d")/sample
            CURRENT_DATE=$(date +"%Y-%m-%d")

            echo "Creating sample data file at performance-reports/$CURRENT_DATE/sample/summary.json"
            echo '{
              "page": "sample",
              "url": "https://example.com",
              "date": "'$CURRENT_DATE'",
              "desktop": {
                "performance": 90,
                "accessibility": 90,
                "bestPractices": 90,
                "seo": 90,
                "pwa": 90,
                "metrics": {
                  "firstContentfulPaint": 1000,
                  "speedIndex": 1500,
                  "largestContentfulPaint": 2000,
                  "timeToInteractive": 2500,
                  "totalBlockingTime": 100,
                  "maxPotentialFID": 100,
                  "cumulativeLayoutShift": 0.05
                },
                "opportunities": {
                  "renderBlockingResources": 0,
                  "unusedCSSBytes": 0,
                  "unusedJSBytes": 0,
                  "offscreenImagesBytes": 0,
                  "totalBytes": 500000,
                  "domSize": 1000
                }
              },
              "mobile": {
                "performance": 85,
                "accessibility": 90,
                "bestPractices": 90,
                "seo": 90,
                "pwa": 90,
                "metrics": {
                  "firstContentfulPaint": 1200,
                  "speedIndex": 1800,
                  "largestContentfulPaint": 2400,
                  "timeToInteractive": 3000,
                  "totalBlockingTime": 150,
                  "maxPotentialFID": 150,
                  "cumulativeLayoutShift": 0.08
                },
                "opportunities": {
                  "renderBlockingResources": 0,
                  "unusedCSSBytes": 0,
                  "unusedJSBytes": 0,
                  "offscreenImagesBytes": 0,
                  "totalBytes": 600000,
                  "domSize": 1000
                }
              }
            }' > performance-reports/$CURRENT_DATE/sample/summary.json
          fi

          # Check if there are any files in the performance-reports directory
          echo "Contents of performance-reports directory:"
          find performance-reports -type f | sort || echo "No files found in performance-reports"

      # Generate the HTML dashboard - split into smaller steps for better error tracking
      - name: Generate HTML dashboard
        run: |
          CURRENT_DATE="${{ needs.lighthouse-ci.outputs.report_date }}"
          if [ -z "$CURRENT_DATE" ]; then
            CURRENT_DATE=$(date +"%Y-%m-%d")
          fi

          echo "Generating dashboard for date: $CURRENT_DATE"

          # Run with bash -e to exit immediately on error
          bash -e .github/workflows/scripts/generate-dashboard.sh "$CURRENT_DATE" || {
            echo "Error generating dashboard, checking for issues..."
            # Create a simple default dashboard if the script fails
            echo "<!DOCTYPE html><html><head><title>Lighthouse Dashboard</title></head><body><h1>Lighthouse Dashboard</h1><p>No valid reports were found or an error occurred during dashboard generation.</p></body></html>" > performance-reports/index.html
            echo "Created fallback dashboard"
          }

          echo "Dashboard file generation complete"

          if [ -f "performance-reports/index.html" ]; then
            echo "Dashboard file exists at performance-reports/index.html"
            ls -la performance-reports/index.html
          else
            echo "Dashboard file was not created!"
          fi

      # Generate the GitHub Step Summary separately for better error handling
      - name: Generate GitHub Step Summary
        run: |
          # Create a basic step summary header
          echo "## üö¶ Lighthouse Performance Dashboard" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìä Summary Results ($(date))" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if there are any summary files to process
          SUMMARY_FILES=$(find performance-reports -name "summary.json" | wc -l)

          if [ "$SUMMARY_FILES" -eq "0" ]; then
            echo "No summary files found to process" >> $GITHUB_STEP_SUMMARY
            echo "No test results are available for this run." >> $GITHUB_STEP_SUMMARY
          else
            echo "Found $SUMMARY_FILES summary files to process"

            # Add a table for desktop results
            echo "#### üíª Desktop Results" >> $GITHUB_STEP_SUMMARY
            echo "| Page | Performance | Accessibility | Best Practices | SEO | LCP | TBT | CLS |" >> $GITHUB_STEP_SUMMARY
            echo "|------|-------------|---------------|----------------|-----|-----|-----|-----|" >> $GITHUB_STEP_SUMMARY

            # Find all summary.json files for desktop
            find performance-reports -name "summary.json" | sort | while read -r file; do
              # Debug
              echo "Processing file: $file"

              # Extract data safely with error handling
              PAGE_NAME=$(grep -o '"page": *"[^"]*"' "$file" | cut -d'"' -f4 || echo "unknown")
              PERF=$(grep -o '"performance": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              A11Y=$(grep -o '"accessibility": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              BP=$(grep -o '"bestPractices": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              SEO=$(grep -o '"seo": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              LCP=$(grep -o '"largestContentfulPaint": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              TBT=$(grep -o '"totalBlockingTime": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              CLS=$(grep -o '"cumulativeLayoutShift": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")

              # Format values for display
              LCP_MS=$(echo "$LCP / 1000" | bc -l 2>/dev/null | xargs printf "%.2fs" 2>/dev/null || echo "0.00s")
              TBT_MS=$(echo "$TBT" | xargs printf "%.0fms" 2>/dev/null || echo "0ms")

              echo "| $PAGE_NAME | $PERF | $A11Y | $BP | $SEO | $LCP_MS | $TBT_MS | $CLS |" >> $GITHUB_STEP_SUMMARY
            done

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "#### üì± Mobile Results" >> $GITHUB_STEP_SUMMARY
            echo "| Page | Performance | Accessibility | Best Practices | SEO | LCP | TBT | CLS |" >> $GITHUB_STEP_SUMMARY
            echo "|------|-------------|---------------|----------------|-----|-----|-----|-----|" >> $GITHUB_STEP_SUMMARY

            # Find all summary.json files for mobile
            find performance-reports -name "summary.json" | sort | while read -r file; do
              # Extract data safely with error handling
              PAGE_NAME=$(grep -o '"page": *"[^"]*"' "$file" | cut -d'"' -f4 || echo "unknown")
              PERF=$(grep -o '"performance": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              A11Y=$(grep -o '"accessibility": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              BP=$(grep -o '"bestPractices": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              SEO=$(grep -o '"seo": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              LCP=$(grep -o '"largestContentfulPaint": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              TBT=$(grep -o '"totalBlockingTime": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")
              CLS=$(grep -o '"cumulativeLayoutShift": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")

              # Format values for display
              LCP_MS=$(echo "$LCP / 1000" | bc -l 2>/dev/null | xargs printf "%.2fs" 2>/dev/null || echo "0.00s")
              TBT_MS=$(echo "$TBT" | xargs printf "%.0fms" 2>/dev/null || echo "0ms")

              echo "| $PAGE_NAME | $PERF | $A11Y | $BP | $SEO | $LCP_MS | $TBT_MS | $CLS |" >> $GITHUB_STEP_SUMMARY
            done

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### üìà Core Web Vitals Status" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- ‚úÖ **Good**: LCP < 2.5s, CLS < 0.1, TBT < 200ms" >> $GITHUB_STEP_SUMMARY
            echo "- ‚ö†Ô∏è **Needs Improvement**: LCP < 4s, CLS < 0.25, TBT < 600ms" >> $GITHUB_STEP_SUMMARY
            echo "- ‚ùå **Poor**: LCP > 4s, CLS > 0.25, TBT > 600ms" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üîç View Full Reports" >> $GITHUB_STEP_SUMMARY
          echo "Download the full HTML reports from the workflow artifacts section." >> $GITHUB_STEP_SUMMARY

      - name: List created files
        run: |
          echo "Files created in performance-reports directory:"
          mkdir -p performance-reports
          find performance-reports -type f | sort || echo "No files found in performance-reports"

      # Upload the complete combined report
      - name: Upload performance reports
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-complete-report
          path: performance-reports/
          retention-days: 30
        continue-on-error: true

      # Generate visual annotations for PRs
      - name: Add PR annotations
        if: github.event_name == 'pull_request'
        run: |
          # Find all summary.json files
          find performance-reports -name "summary.json" | sort | while read -r file; do
            PAGE_NAME=$(grep -o '"page": *"[^"]*"' "$file" | cut -d'"' -f4 || echo "unknown")
            PERF=$(grep -o '"performance": *[0-9.]*' "$file" | grep -o '[0-9.]*' || echo "0")

            # Add annotations based on performance score
            if (( $(echo "$PERF < 75" | bc -l) )); then
              echo "::warning file=${PAGE_NAME}::Performance score is poor: ${PERF}%. Consider optimizing this page."
            elif (( $(echo "$PERF < 90" | bc -l) )); then
              echo "::notice file=${PAGE_NAME}::Performance score needs improvement: ${PERF}%."
            else
              echo "::notice file=${PAGE_NAME}::Performance score is good: ${PERF}%."
            fi
          done
        continue-on-error: true
